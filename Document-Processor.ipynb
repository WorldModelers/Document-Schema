{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing hackathon documents to Elasticsearch\n",
    "\n",
    "This notebook indexes the 358 June, 2019 hackathon documents to Elasticsearch. Each document is stored with its corresponding metadata.\n",
    "\n",
    "\n",
    "### Configuration\n",
    "First, ensure that the appropriate credentials are stored in your AWS credentials at `~/.aws/credentials`.\n",
    "\n",
    "These should be stored under the `wmuser` profile with something like:\n",
    "\n",
    "```\n",
    "[wmuser]\n",
    "aws_access_key_id = WMUSER_ACCESS_KEY\n",
    "aws_secret_access_key = WMUSER_SECRET_KEY\n",
    "```\n",
    "\n",
    "> Note that this profile must be specified by name when creating the `boto3` session.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "```\n",
    "pip install requests-aws4auth==0.9\n",
    "pip install elasticsearch==7.0.2\n",
    "pip install tika==1.19\n",
    "pip install PyPDF2==1.26.0\n",
    "pip install boto3==1.9.172\n",
    "pip install beautifulsoup4==4.5.3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to Elasticsearch\n",
    "First we should connect to Elasticsearch using AWS authentification. This will make it easy to index each parsed document later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/p37/lib/python3.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.25.3) or chardet (2.3.0) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import boto3, json\n",
    "import os\n",
    "from shutil import copyfile\n",
    "from hashlib import sha256\n",
    "from elasticsearch import Elasticsearch, RequestsHttpConnection\n",
    "from requests_aws4auth import AWS4Auth\n",
    "from tika import parser\n",
    "import PyPDF2\n",
    "from bs4 import BeautifulSoup\n",
    "from jsonschema import validate\n",
    "\n",
    "region = 'us-east-1'\n",
    "service = 'es'\n",
    "eshost = 'search-world-modelers-dev-gjvcliqvo44h4dgby7tn3psw74.us-east-1.es.amazonaws.com'\n",
    "\n",
    "session = boto3.Session(region_name=region, profile_name='wmuser')\n",
    "credentials = session.get_credentials()\n",
    "credentials = credentials.get_frozen_credentials()\n",
    "access_key = credentials.access_key\n",
    "secret_key = credentials.secret_key\n",
    "token = credentials.token\n",
    "\n",
    "aws_auth = AWS4Auth(\n",
    "    access_key,\n",
    "    secret_key,\n",
    "    region,\n",
    "    service,\n",
    "    session_token=token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"ZhaR9MU\",\n",
      "  \"cluster_name\": \"342635568055:world-modelers-dev\",\n",
      "  \"cluster_uuid\": \"nGeAO1lMTKaG6_LOpSg17w\",\n",
      "  \"version\": {\n",
      "    \"number\": \"6.7.0\",\n",
      "    \"build_flavor\": \"oss\",\n",
      "    \"build_type\": \"zip\",\n",
      "    \"build_hash\": \"8453f77\",\n",
      "    \"build_date\": \"2019-04-17T05:34:35.022392Z\",\n",
      "    \"build_snapshot\": false,\n",
      "    \"lucene_version\": \"7.7.0\",\n",
      "    \"minimum_wire_compatibility_version\": \"5.6.0\",\n",
      "    \"minimum_index_compatibility_version\": \"5.0.0\"\n",
      "  },\n",
      "  \"tagline\": \"You Know, for Search\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "es = Elasticsearch(\n",
    "    hosts = [{'host': eshost, 'port': 443}],\n",
    "    http_auth=aws_auth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection\n",
    ")\n",
    "\n",
    "print(json.dumps(es.info(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to S3\n",
    "Next we should establish a connection with the S3 `world-modelers` bucket so that we can store each file to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = \"wmuser\"\n",
    "bucket_name = \"world-modelers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(profile_name=profile)\n",
    "\n",
    "s3 = session.resource(\"s3\")\n",
    "s3_client = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Parsing Functions\n",
    "Below are a set of functions to extract text from PDF and HTML and to extract appropriate metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tika(file_path):\n",
    "    \"\"\"\n",
    "    Take in a file path of a PDF and return its Tika extraction\n",
    "    https://github.com/chrismattmann/tika-python\n",
    "    \n",
    "    Returns: a tuple of (extracted text, extracted metadata)\n",
    "    \"\"\"\n",
    "    tika_data = parser.from_file(file_path)\n",
    "    tika_extraction = tika_data.pop('content')\n",
    "    tika_metadata = tika_data.pop('metadata')\n",
    "    return (tika_extraction, tika_metadata)\n",
    "\n",
    "def extract_pypdf2(file_path):\n",
    "    \"\"\"\n",
    "    Take in a file path of a PDF and return its PyPDF2 extraction\n",
    "    https://github.com/mstamy2/PyPDF2\n",
    "    \"\"\"\n",
    "    \n",
    "    pdfFileObj = open(file_path, 'rb')\n",
    "    pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "    page_count = pdfReader.numPages\n",
    "    pypdf2_extraction = ''\n",
    "    for page in range(page_count):\n",
    "        pageObj = pdfReader.getPage(page)\n",
    "        page_text = pageObj.extractText()\n",
    "        pypdf2_extraction += page_text\n",
    "    return pypdf2_extraction\n",
    "\n",
    "def extract_bs4(file_path):\n",
    "    \"\"\"\n",
    "    Take in a file path of an HTML document and return its Beautiful Soup extraction\n",
    "    https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "    \"\"\"        \n",
    "    htmlFileObj = open(file_path, 'r')\n",
    "    soup = BeautifulSoup(htmlFileObj, \"lxml\")\n",
    "    # kill all script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()    # rip it out\n",
    "    # get text\n",
    "    text = soup.get_text()        \n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    bs4_extraction = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    return bs4_extraction\n",
    "\n",
    "def parse_pdfinfo(tika_metadata, doc):\n",
    "    \"\"\"\n",
    "    Takes in pdfinfo from Tika and a document and enriches the document\n",
    "    with metadata fields\n",
    "    \"\"\"\n",
    "    t_m = extract_tika(f\"{dir_path}/pdf/{file_path}\")[1]\n",
    "    title = t_m.get('title',None)\n",
    "    date = t_m.get('Creation-Date',t_m.get('created',None))\n",
    "    author = t_m.get('Author',None)\n",
    "    last_modified = t_m.get('Last-Modified',None)\n",
    "    if title:\n",
    "        doc['title'] = title\n",
    "    if date:\n",
    "        doc['creation_date'] = {'date': date}\n",
    "    if author:\n",
    "        doc['source'] = {'author_name': author}\n",
    "    if last_modified:\n",
    "        doc['modification_date'] = {'date': last_modified}\n",
    "    return doc\n",
    "\n",
    "def parse_document(file_path, category, source_url):\n",
    "    \"\"\"\n",
    "    Take in the full path to a file and perform appropriate text extrraction\n",
    "    as well as metadata enrichment (if a PDF, using pdfinfo fields)\n",
    "    \"\"\"\n",
    "    file_name = os.path.basename(file_path)\n",
    "    file_type = os.path.splitext(file_path)[1]\n",
    "    \n",
    "    # sha256 hash the raw contents of the file to generate a UUID\n",
    "    raw = open(file_path,'rb').read()\n",
    "    _id = sha256(raw).hexdigest()\n",
    "    \n",
    "    doc = {'_id': _id,\n",
    "           'file_name': file_name, \n",
    "           'file_type': file_type,\n",
    "           'category': category,\n",
    "           'source_url': source_url}\n",
    "    \n",
    "    extracted_text = {}\n",
    "    \n",
    "    # set tika_metadata to None and overwrite it\n",
    "    # if we are able to extract pdfinfo with Tika\n",
    "    tika_metadata = None\n",
    "    \n",
    "    if file_type == '.pdf':\n",
    "        doc['file_type'] = file_type\n",
    "        try:\n",
    "            tika_extraction, tika_metadata = extract_tika(file_path)\n",
    "            extracted_text['tika'] = tika_extraction\n",
    "        except:\n",
    "            # need to strip random unicode from file_path so store a tmp file using the \n",
    "            # documents _id\n",
    "            try:\n",
    "                copyfile(file_path, f'/Users/brandon/Desktop/Docs_20-May-2019/tmp/{_id}.pdf')\n",
    "                extract_tika(f'/Users/brandon/Desktop/Docs_20-May-2019/tmp/{_id}.pdf')\n",
    "            except Exception as e:\n",
    "                print(f\"Tika extraction failed: {e}\") \n",
    "        try:\n",
    "            extracted_text['pypdf2'] = extract_pypdf2(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"PyPDF2 extraction failed: {e}\")\n",
    "    elif file_type == '.html':\n",
    "        try:\n",
    "            extracted_text['bs4'] = extract_bs4(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"BS4 extraction failed: {e}\")\n",
    "    \n",
    "    if tika_metadata:\n",
    "        doc = parse_pdfinfo(tika_metadata, doc)\n",
    "    \n",
    "    doc['extracted_text'] = extracted_text\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing documents\n",
    "Now that we have a connection with Elasticsearch and S3 we can index the documents. \n",
    "\n",
    "1. First, we will push the raw file (PDF or HTML) to S3.\n",
    "2. Then will first perform text and metadata extraction in order to create a document to index.\n",
    "3. Finally, we will index the document to Elasticsearch.\n",
    "\n",
    "> Note: you must update the `dir_path` to the appropriate path to the hackathon documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update this path with the correct path to the hackathon documents\n",
    "dir_path = '/Users/brandon/Desktop/Docs_20-May-2019'\n",
    "raw_files = os.listdir(dir_path + '/pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when we index the document we pop its `_id` and store that as its Elasticsearch `_id`. This ensures that if we index a document to that `_id` we will be updating the document in place, not generating a new document in Elasticsearch.\n",
    "\n",
    "Since each document was hashed to generate an `_id`, of the 358 original documents there were at least 2 exact duplicates; once hashed there were 356 unique documents.\n",
    "\n",
    "We can also use the `document-schema.json` to validate each document we process to ensure schema conformity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = json.loads(open(\"document-schema.json\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for file_path in raw_files:    \n",
    "    \n",
    "    file_name = f\"{dir_path}/pdf/{file_path}\"\n",
    "    s3_key = f\"documents/migration/{file_path}\"\n",
    "    s3_uri = f\"https://world-modelers.s3.amazonaws.com/{s3_key}\"\n",
    "\n",
    "    \n",
    "    #############################################\n",
    "    ### 1. Upload raw file to S3 ################\n",
    "    #############################################\n",
    "    s3_client.upload_file(file_name, \n",
    "                          bucket_name, \n",
    "                          s3_key)\n",
    "    \n",
    "    \n",
    "    #############################################\n",
    "    ### 2. Parse document #######################\n",
    "    #############################################\n",
    "    # hard code category and source_url (empty) for the time being\n",
    "    category = 'Migration'\n",
    "    source_url = ''\n",
    "    doc = parse_document(file_name, category, source_url)\n",
    "    doc['stored_url'] = s3_uri\n",
    "    \n",
    "    # Validate document against schema\n",
    "    validate(instance=doc, schema=schema)\n",
    "\n",
    "    \n",
    "    #############################################\n",
    "    ### 3. Index parsed document to Elasticsearch\n",
    "    #############################################  \n",
    "    index = 'wm-dev'\n",
    "    doc_type = 'wm-document'\n",
    "    \n",
    "    # create the index if it does not exist\n",
    "    if not es.indices.exists(index):\n",
    "        es.indices.create(index)\n",
    "        print(f\"Created ES index: {index}\")\n",
    "        \n",
    "    es.index(index=index, doc_type=doc_type, id=doc.pop('_id'), body=doc)\n",
    "    count += 1\n",
    "    if count % 25 == 0:\n",
    "        print(count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
